1)Як ви робите дебагінг (якщо є проблема, як ви її вирішуєте, які інструменти використовуєте, запишіть порядок дій?

Найбільш базовим і зрозумілим інструментом для відладки, як на мене є логування. 
Особливо під час локальної розробки дебаг за допомогою console.log часто буває дуже зручним і дозволяє швидко зрозуміти, що відбувається в програмі.
Якщо потрібно виконувати відладку покроково та аналізувати стан програми на конкретному етапі виконання, 
я використовую дебаггер, який вбудований в IDE. Наприклад, у WebStorm є дуже зручний дебаггер із коробки, 
який дозволяє ставити breakpoints, переглядати значення змінних і контролювати виконання коду крок за кроком.
Що стосується проду, там вже відштовхуюсь від того де воно розгорнуто, якщо це наприклад EC2 instance
можна зайти в середину VM і подивитись як біжать логи прямо на сервері. 
Також можна використовувати стандартні інструменти для профілювання типу Logs.io aбо DataDog якщо вони були попередньо налаштовані,
там зазвичай є можливість  і  Memory Leak  подивитись якщо мова йде про DataDog

2. Що таке Оптимізаційний скіл?
Перше, що спадає на думку, — це оптимізація коду. Наприклад, коли ми покращуємо алгоритм і зменшуємо його складність, умовно, з O(n²) до O(n). 
Це дозволяє значно підвищити продуктивність і ефективність роботи програми.
Також оптимізаційний скіл може стосуватися не лише коду, а й баз даних. Наприклад, на моєму останньому проєкті була ситуація, 
коли просте додавання індексу суттєво зменшило навантаження на базу даних.
До цього використання ресурсів було на рівні майже 100%, і система працювала на межі своїх можливостей. Після додавання індексу навантаження значно зменшилось і стабільно трималося на рівні не більше 20%.
Крім того, оптимізація — це не завжди лише про прискорення роботи програми. Іноді оптимізацією може бути звичайний рефакторинг коду. Наприклад, коли велику, складну і громіздку функцію розбивають на кілька менших, виносять перевикористовувану логіку в окремі частини. Це покращує читабельність коду, спрощує його підтримку і дозволяє витрачати менше часу на його розуміння в майбутньому.
Також можна додати GH Copilot для ревью кода і зекономити  команді час, бо він вкаже на очевидні базові помилки
Можна аі-шку використовувати для того щоб писати код швидше або наприклад написати не оптимальний код скинути в AI і сказати 'знайди оптимальне рішення'



3) Розкажіть про свій досвід роботи з Linux-серверами, виконання базових адміністративних завдань?
Із базових адміністративних завдань я працював із розгортанням застосунків на Linux-серверах, зокрема в AWS. 
Наприклад, я піднімав EC2 instance, після чого підключався до нього через SSH, клонував репозиторій із Git за допомогою командного рядка розгортав проєкт на VM.
Також я працював із розгортанням баз даних. 
У залежності від архітектури, базу даних можна піднімати безпосередньо на сервері або використовувати керовані cloud-рішення, наприклад AWS RDS. 
У Google Cloud я також використовував їхні керовані сервіси для розгортання інфраструктури та баз даних.
Крім цього, я виконував базові задачі адміністрування, такі як підключення до віддаленого сервера через SSH, 
перегляд логів застосунку безпосередньо на сервері, якщо не використовуються сторонні сервіси логування, а також запуск і перевірка роботи застосунку в серверному середовищі.
А також інші дрібні задачі типу створити файл .env
sudo nano .env
прочитати cat some.js, скопіювати  і тд
4) Що таке асинхронність в Node.js? 
Ps. відповідав сам просто просив чат GPT щоб він розставив коми і тд. щоб було зручніше читати.
Асинхронність у Node.js — це можливість виконувати неблокуючі операції, що дозволяє обробляти інші задачі, 
не очікуючи завершення довготривалих операцій, таких як робота з файловою системою, мережею або базою даних. 
Це є ключовою особливістю Node.js, яка дозволяє ефективно обробляти велику кількість одночасних запитів в одному потоці.
Асинхронність у Node.js реалізована за допомогою Event Loop — механізму, який керує виконанням асинхронних задач і визначає порядок їх обробки. 
Коли асинхронна операція запускається, вона передається на виконання в libuv — бібліотеку, 
яка відповідає за роботу з системними ресурсами, такими як мережа, файлові операції та thread pool.
Після завершення операції відповідний callback або Promise додається в чергу подій, і Event Loop виконує його, коли стек викликів стає вільним.
Також важливу роль відіграють мікротаски, такі як Promise callbacks і process.nextTick. 
Вони мають вищий пріоритет і виконуються раніше, ніж наступна ітерація Event Loop і macrotasks, що дозволяє більш точно керувати порядком виконання асинхронного коду.
Для роботи з асинхронністю в Node.js використовуються такі підходи, як callbacks, Promises і async/await, які дозволяють писати неблокуючий код і ефективно керувати виконанням асинхронних операцій. Це дозволяє Node.js забезпечувати високу продуктивність і масштабованість серверних застосунків.
Про всяк випадок додам схемку роботи івент лупа)) бо це ж по суті і є основа асинхронності в Ноді
5) Як допомагає в щоденній роботі штучний інтелект, чи є приклади коли ШІ відчутно зекономив час?
 вище вже навів приклад, але тут трохи додам 
зручно з АІшкою робити рутину, типу який небудь цикл важкий писати, вона суттєво зберігає  час 
або використати замість качки коли намагаєшься вирішити задачу яку не зрозуміло як вирішувати: 
пояснюєш AI, що потрібно зробити, і при цьому сам краще розумієш суть завдання. 
Часто він не тільки виконує те, що ти просиш, а й може підкинути корисну ідею чи нестандартне рішення. 
Навіть якщо AI спершу робить щось неправильно, процес пояснення, як це треба зробити правильно, допомагає самому зрозуміти алгоритм і підхід до задачі. 
6) 
Логування – найважливіше. 
Через логи можна відслідковувати кількість запитів, частоту їх надходження та інші показники, що допомагає швидко локалізувати проблему.
Стандартні утиліти Node.js – наприклад, process.memoryUsage(), heapUsed, heapTotal та інші, 
які дозволяють моніторити стан пам’яті і ресурси сервера. але іх не вікористовував мало 
Моніторинг і візуалізація – активно використовували зв’язку Prometheus + Grafana, яка добре показувала, 
скільки запитів виконується до бази даних, як часто вони надходять і які процеси створюють навантаження.
Сервісні інструменти хмарної платформи – якщо проект розгорнутий у Google Cloud, 
можна використовувати стандартні інструменти профілювання для баз даних: CPU utilization, аналіз важких запитів, що допомагає визначити вузькі місця і планувати оптимізацію.
Додаткові інструменти – такі як Datadog, а також підходи до масштабування:
додавання індексів, збільшення потужності серверів, підняття кількох інстансів, 
кешування через Redis або кешування відповідей API.



